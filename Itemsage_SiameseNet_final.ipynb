{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a11487f-3ef7-4eca-ad39-ef8c93ce5611",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting petastorm\n",
      "  Downloading petastorm-0.12.1-py2.py3-none-any.whl (284 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.0/284.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyarrow>=0.17.1 in /opt/zillow/.venv/lib/python3.9/site-packages (from petastorm) (11.0.0)\n",
      "Requirement already satisfied: fsspec in /opt/zillow/.venv/lib/python3.9/site-packages (from petastorm) (2023.3.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/zillow/.venv/lib/python3.9/site-packages (from petastorm) (1.23.5)\n",
      "Requirement already satisfied: dill>=0.2.1 in /opt/zillow/.venv/lib/python3.9/site-packages (from petastorm) (0.3.6)\n",
      "Requirement already satisfied: packaging>=15.0 in /opt/zillow/.venv/lib/python3.9/site-packages (from petastorm) (23.0)\n",
      "Requirement already satisfied: pandas>=0.19.0 in /opt/zillow/.venv/lib/python3.9/site-packages (from petastorm) (1.5.3)\n",
      "Collecting diskcache>=3.0.0\n",
      "  Downloading diskcache-5.6.1-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 kB\u001b[0m \u001b[31m602.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil>=4.0.0 in /opt/zillow/.venv/lib/python3.9/site-packages (from petastorm) (5.9.4)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/zillow/.venv/lib/python3.9/site-packages (from petastorm) (1.16.0)\n",
      "Requirement already satisfied: pyzmq>=14.0.0 in /opt/zillow/.venv/lib/python3.9/site-packages (from petastorm) (25.0.2)\n",
      "Collecting future>=0.10.2\n",
      "  Downloading future-0.18.3.tar.gz (840 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.9/840.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyspark>=2.1.0\n",
      "  Using cached pyspark-3.4.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/zillow/.venv/lib/python3.9/site-packages (from pandas>=0.19.0->petastorm) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/zillow/.venv/lib/python3.9/site-packages (from pandas>=0.19.0->petastorm) (2022.7.1)\n",
      "Collecting py4j==0.10.9.7\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Building wheels for collected packages: future\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492035 sha256=6773641352a8ff5b4d135a7c399067030207253a06294906304ee2dbc6ffd6da\n",
      "  Stored in directory: /home/shreyasve/.cache/pip/wheels/bf/5d/6a/2e53874f7ec4e2bede522385439531fafec8fafe005b5c3d1b\n",
      "Successfully built future\n",
      "Installing collected packages: py4j, pyspark, future, diskcache, petastorm\n",
      "Successfully installed diskcache-5.6.1 future-0.18.3 petastorm-0.12.1 py4j-0.10.9.7 pyspark-3.4.1\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install petastorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d86321c-95da-41ac-9c2a-a0ff5f6afec2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'petastorm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sample\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpetastorm\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpetastorm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpetastorm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_reader\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'petastorm'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pyarrow.parquet import ParquetFile\n",
    "import pyarrow.parquet as pq\n",
    "# import s3fs\n",
    "# s3_filesystem = s3fs.S3FileSystem()\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import ast\n",
    "import time\n",
    "import numpy as np\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from random import sample\n",
    "from tqdm import tqdm\n",
    "import petastorm\n",
    "from petastorm.pytorch import DataLoader\n",
    "from petastorm import make_reader\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1caf2463-1ada-474c-b4b3-29264e0fd263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLING_RATIO = 3 # BATCH_SIZE_NEG : BATCH_SIZE_POS \n",
    "# BATCH_SIZE_POS = 16\n",
    "# BATCH_SIZE_NEG = BATCH_SIZE_POS * SAMPLING_RATIO\n",
    "os.chdir(os.path.join(os.path.expanduser(\"~\"),'ItemSage_Data_v3'))\n",
    "home_dir = os.getcwd()\n",
    "TRAIN_PATH = 'train3_img_flattened'\n",
    "# TEST_PATH = 'test_img_flattened'\n",
    "VAL_PATH = 'val3_img_flattened'\n",
    "TRAIN_SIZE = 1757020\n",
    "NUM_EPOCHS = 25\n",
    "ENGAGEMENT_TYPE = 'all' # clicks/saves/all\n",
    "\n",
    "LOG_INTERVAL = 1300 # just to keep ~10 reporting logs per epoch\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "HOME_FEATS_DIM = 30\n",
    "IMAGE_INPUT_EMBED_DIM = 512 \n",
    "NUM_IMAGES = 10\n",
    "TEXT_INPUT_EMBED_DIM = 1536 \n",
    "SKG_INPUT_EMBED_DIM = 32  # skip-gram\n",
    "PS_INPUT_EMBED_DIM = 128 # pinsage\n",
    "TRANSFORMER_INPUT_DIM = 512\n",
    "ITEMSAGE_EMBED_DIM = 256\n",
    "N_ZIP_CATEGORIES = 14 # look this up everytime while creating data\n",
    "ZIP_EMBED_DIM = 16\n",
    "\n",
    "LOSS_TYPE = \"contrastive\" # contrastive/classification/itemsage_loss\n",
    "# LOSS_FUNC = nn.BCELoss # classification : BCELoss() | contrastive : nn.CosineEmbeddingLoss() | itemsage_loss : simCLRLoss() [custom - to be implemented]\n",
    "CONTRASTIVE_LOSS_MARGIN = 0.1\n",
    "DEVICE = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61de14c8-2628-49ec-8bf0-99199476d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = {\n",
    "    'n_epochs' : NUM_EPOCHS,\n",
    "    'log_interval' : LOG_INTERVAL,\n",
    "    'bsz':BATCH_SIZE,\n",
    "    'engagement_type' : ENGAGEMENT_TYPE,\n",
    "    'loss_type': LOSS_TYPE,\n",
    "    'loss' : nn.BCELoss if LOSS_TYPE == 'classification' else nn.CosineEmbeddingLoss,\n",
    "    'loss_margin':None if LOSS_TYPE == 'classification' else CONTRASTIVE_LOSS_MARGIN,\n",
    "    'home_features_dim' : HOME_FEATS_DIM,\n",
    "    'image_embed_dim' : IMAGE_INPUT_EMBED_DIM,\n",
    "    'n_img' : NUM_IMAGES,\\\n",
    "    'text_embed_dim' : TEXT_INPUT_EMBED_DIM,\n",
    "    'skg_embed_dim' : SKG_INPUT_EMBED_DIM,\n",
    "    'ps_embed_dim' : PS_INPUT_EMBED_DIM,\n",
    "    'transformer_input_dim' : TRANSFORMER_INPUT_DIM,\n",
    "    'final_embed_dim' : ITEMSAGE_EMBED_DIM,\n",
    "    'n_zips':N_ZIP_CATEGORIES,\n",
    "    'zip_embed_dim' : ZIP_EMBED_DIM,\n",
    "    'device' : DEVICE,\n",
    "    'train_path':''.join(['file://',os.path.join(home_dir, TRAIN_PATH)]),\n",
    "    # 'test_path':''.join(['file://',os.path.join(home_dir, TEST_PATH)]),\n",
    "    'val_path':''.join(['file://',os.path.join(home_dir, VAL_PATH)]),\n",
    "    'model_path': os.path.join(home_dir, 'Models'),\n",
    "    'train_size' : TRAIN_SIZE\n",
    "}\n",
    "\n",
    "cuda_args = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "\n",
    "model_args = {**model_args, **cuda_args}\n",
    "\n",
    "model_save_dir = Path(os.path.join(model_args['model_path'],'_'.join(['model',LOSS_TYPE,datetime.now().strftime('%Y-%m-%d_%H-%M-%S')])))\n",
    "if not model_save_dir.exists():\n",
    "        model_save_dir.mkdir(parents=True)\n",
    "        \n",
    "log_file = os.path.join(model_save_dir,'output.log')\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5356700-bf96-4d0e-a486-ad3cade5fac4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## TODO : 1. Add Positional Encoding  - the one in Attention paper [sinusoidal,static]\n",
    "#                                     - as a trainable encoding (?) - check this out, seems interesting\n",
    "# dont know how much it would make sense - lets try it out and see if there is any diff in perf\n",
    "\n",
    "# SIAMESE NETWORK IMPLEMENTATION FOR ITEMSAGE\n",
    "\n",
    "class ItemSageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, nhead = 8,nlayers = 1,**kwargs):\n",
    "        super(ItemSageModel,self).__init__()\n",
    "        \n",
    "        home_feat_dim = kwargs['home_features_dim']\n",
    "        image_embedding_dim = kwargs['image_embed_dim']\n",
    "        text_embedding_dim = kwargs['text_embed_dim']\n",
    "        skg_embedding_dim = kwargs['skg_embed_dim']\n",
    "        ps_embedding_dim = kwargs['ps_embed_dim']\n",
    "        transformer_input_dim = kwargs['transformer_input_dim']\n",
    "        output_dim = kwargs['final_embed_dim']\n",
    "        n_zips = kwargs['n_zips']\n",
    "        zip_embed_dim = kwargs['zip_embed_dim']\n",
    "        \n",
    "        if kwargs['loss_margin'] :\n",
    "            self.loss = kwargs['loss'](margin=kwargs['loss_margin']) # criterion created\n",
    "        else:\n",
    "            self.loss = kwargs['loss']( )\n",
    "            \n",
    "        self.device = kwargs['device']\n",
    "        \n",
    "        # converting all input to the same dimensionality for forming transformer input seq\n",
    "        self.zip_embed = nn.Embedding(n_zips,zip_embed_dim)\n",
    "        self.home_linear = nn.Linear(home_feat_dim+zip_embed_dim, transformer_input_dim)\n",
    "        self.image_linear = nn.Linear(image_embedding_dim, transformer_input_dim)\n",
    "        self.text_linear = nn.Linear(text_embedding_dim, transformer_input_dim)\n",
    "        self.skg_linear = nn.Linear(skg_embedding_dim, transformer_input_dim)\n",
    "        self.ps_linear = nn.Linear(ps_embedding_dim, transformer_input_dim)\n",
    "        \n",
    "        # transforming the CLS token\n",
    "        self.global_CLS_linear = nn.Linear(1, transformer_input_dim)\n",
    "        \n",
    "        # Transformer Encoder Layer\n",
    "        self.encoder_layer = TransformerEncoderLayer(d_model=transformer_input_dim, nhead=nhead,\\\n",
    "                                                     batch_first=True,norm_first=True)\n",
    "        self.transformer_encoder = TransformerEncoder(self.encoder_layer, num_layers=nlayers)\n",
    "        \n",
    "        # MLP head\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(transformer_input_dim),\n",
    "            nn.Linear(transformer_input_dim, 1024),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(1024, output_dim)\n",
    "        )\n",
    "    \n",
    "    # option to use one of the two loss types\n",
    "    def _contrastive_loss(self, x1,x2,label):\n",
    "        return self.loss(x1,x2,label)\n",
    "    \n",
    "    def _classification_loss(self,score,label):\n",
    "        return self.loss(score,label)\n",
    "\n",
    "    # itemsage tower\n",
    "    def _encoder(self, text_embeddings, image_embeddings, skg_embeddings, ps_embeddings, home_features, zip_idx):\n",
    "        \"\"\" \n",
    "        :param image_embeddings: (bsz, seq_length, D_img) - \n",
    "        :param text_embeddings: (bsz,  D_text) - make seq_length = 1 for openAI embeddings - using unsqueeze()\n",
    "        :param skg_embeddings: (bsz, D_skg) - make seq_length = 1 because 1 embedding/listing - using unsqueeze()\n",
    "        :param ps_embeddings: (bsz, D_ps) - make seq_length = 1 because 1 embedding/listing - using unsqueeze()\n",
    "        :param home_features: (bsz, D_home_feat) - make seq_length = 1 because 1 embedding/listing - using unsqueeze()\n",
    "        \"\"\"\n",
    "        transformed_image_embeddings = self.image_linear(image_embeddings)\n",
    "        transformed_text_embeddings = self.text_linear(torch.unsqueeze(text_embeddings,1))\n",
    "        transformed_skg_embeddings = self.skg_linear(torch.unsqueeze(skg_embeddings,1))\n",
    "        transformed_ps_embeddings = self.ps_linear(torch.unsqueeze(ps_embeddings,1))\n",
    "        zip_embedding = self.zip_embed(zip_idx)\n",
    "        home_features = torch.cat((home_features,zip_embedding),dim=1)\n",
    "        transformed_home_feat = self.home_linear(torch.unsqueeze(home_features,1))\n",
    "        \n",
    "        # Apply linear transformation to the global token\n",
    "        batch_size = transformed_image_embeddings.size(0)\n",
    "        cls_token = torch.ones(batch_size, 1,1).to(self.device)\n",
    "\n",
    "        \n",
    "        # Apply linear transformation to make [CLS] token 512-dimensional\n",
    "        cls_token = self.global_CLS_linear(cls_token)\n",
    "        \n",
    "        # print(cls_token.shape, transformed_image_embeddings.shape, transformed_text_embeddings.shape)\n",
    "        # embedding sequence\n",
    "        embedding_seq = torch.cat((cls_token, transformed_image_embeddings, transformed_text_embeddings,\\\n",
    "                                   transformed_skg_embeddings,transformed_ps_embeddings,transformed_home_feat), dim=1)\n",
    "        # print('embedding_seq shape : ',embedding_seq.shape)\n",
    "        transformer_output = self.transformer_encoder(embedding_seq)\n",
    "        # print('transformer_output shape : ',transformer_output.shape)\n",
    "        cls_output = transformer_output[:, 0, :]\n",
    "        product_embedding = self.mlp_head(cls_output)\n",
    "        return product_embedding\n",
    "    \n",
    "    \n",
    "    def forward(self,**kwargs1):\n",
    "        anchor_encoder = self._encoder(kwargs1['anchor'][0],kwargs1['anchor'][1],kwargs1['anchor'][2],\\\n",
    "                                      kwargs1['anchor'][3],kwargs1['anchor'][4],kwargs1['anchor'][5]) # query tower\n",
    "        \n",
    "        engaged_encoder = self._encoder(kwargs1['engaged'][0],kwargs1['engaged'][1],kwargs1['engaged'][2],\\\n",
    "                                       kwargs1['engaged'][3],kwargs1['engaged'][4],kwargs1['engaged'][5]) # example tower\n",
    "        \n",
    "        # normalize both embeddings so that cosine_similarity == dot_product\n",
    "        return F.normalize(anchor_encoder,dim=1),F.normalize(engaged_encoder,dim=1)\n",
    "    \n",
    "    \n",
    "class Metric:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, outputs, target, loss):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def reset(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def value(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def name(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "\n",
    "class AccumulatedAccuracyMetric(Metric):\n",
    "    \"\"\"\n",
    "    Works with classification model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,margin=0.):\n",
    "        self.correct = 0\n",
    "        self.total = 0\n",
    "        self.margin = margin\n",
    "\n",
    "    def __call__(self, q_emb,p_emb, labels):\n",
    "        \n",
    "        labels = torch.where (labels == -1,0,labels)\n",
    "        if self.margin:\n",
    "            predicted_prob = F.sigmoid(F.cosine_similarity(q_emb,p_emb) - self.margin)\n",
    "        else:\n",
    "            predicted_prob = F.sigmoid(F.cosine_similarity(q_emb,p_emb))\n",
    "        pred = torch.where (predicted_prob >=0.5,1,0)\n",
    " \n",
    "        self.correct += pred.eq(labels).cpu().sum()\n",
    "        self.total += len(labels)\n",
    "        return self.value()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.correct = 0\n",
    "        self.total = 0\n",
    "\n",
    "    def value(self):\n",
    "        return 100 * float(self.correct) / self.total\n",
    "\n",
    "    def name(self):\n",
    "        return 'Accuracy'\n",
    "    \n",
    "class AccumulatedF1Metric(Metric):\n",
    "    \"\"\"\n",
    "    Works with classification model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,margin=0.):\n",
    "        self.tp,self.fn,self.fp,self.tn = 0,0,0,0\n",
    "        # self.total = 0\n",
    "        self.margin = margin\n",
    "\n",
    "    def __call__(self, q_emb,p_emb, labels):\n",
    "        \n",
    "        labels = torch.where (labels == -1,0,labels)\n",
    "        if self.margin:\n",
    "            predicted_prob = F.sigmoid(F.cosine_similarity(q_emb,p_emb) - self.margin)\n",
    "            # pred = torch.where (predicted_prob >=0.5,1,-1)\n",
    "        else:\n",
    "            predicted_prob = F.sigmoid(F.cosine_similarity(q_emb,p_emb))\n",
    "            \n",
    "        pred = torch.where (predicted_prob >=0.5,1,0)\n",
    "        confusion_vector = pred / labels\n",
    "        self.tp +=  torch.sum(confusion_vector == 1).item()\n",
    "        self.fp +=  torch.sum(confusion_vector == float('inf')).item()\n",
    "        self.tn +=  torch.sum(torch.isnan(confusion_vector)).item()\n",
    "        self.fn +=  torch.sum(confusion_vector == 0).item()\n",
    "\n",
    "        return self.value()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.tp,self.fn,self.fp,self.tn = 0,0,0,0\n",
    "\n",
    "    def value(self):\n",
    "        precision = self.tp/(self.tp+self.fp)\n",
    "        recall = self.tp/(self.tp+self.fn)\n",
    "        return ( 2*precision*recall ) / ( recall + precision )\n",
    "\n",
    "    def name(self):\n",
    "        return 'F1-score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "261f4b33-af78-4567-8dce-d95753a73894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch,train_loader,model,optimizer,**kwargs):\n",
    "    # model = kwargs['model']\n",
    "    model.train()\n",
    "    \n",
    "    log_interval = kwargs['log_interval']\n",
    "    # bsz = kwargs['bsz']\n",
    "    device = kwargs['device']\n",
    "    loss_type = kwargs['loss_type']\n",
    "    metrics = kwargs['metrics']\n",
    "    len_train_data = kwargs['train_size']\n",
    "    for metric in metrics:\n",
    "        metric.reset()\n",
    "    train_loss = 0.\n",
    "    losses = []\n",
    "    \n",
    "    # progress_bar = tqdm(total=BATCH_SIZE_POS, desc='Training Progress')\n",
    "    \n",
    "    counter = 0\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        kwargs1 = {'anchor':[],'engaged':[]}\n",
    "        \n",
    "        #labels\n",
    "        batch_labels = batch['label'].to(device)\n",
    "        bsz = len(batch_labels)\n",
    "        \n",
    "        ## anchor tower input\n",
    "        kwargs1['anchor']= [batch['text_embed1'].to(device),\\\n",
    "                            batch['img_embed1'].view(bsz,kwargs['n_img'],kwargs['image_embed_dim']).to(device),\\\n",
    "                            batch['skg_embed1'].to(device),\\\n",
    "                            batch['ps_embed1'].to(device),\\\n",
    "                            batch['home_features1'][:,:kwargs['home_features_dim']].to(device),\\\n",
    "                            batch['home_features1'][:,kwargs['home_features_dim']].to(torch.long).to(device)]\n",
    "        ## engaged tower input\n",
    "        kwargs1['engaged']= [batch['text_embed2'].to(device),\\\n",
    "                             batch['img_embed2'].view(bsz,kwargs['n_img'],kwargs['image_embed_dim']).to(device),\\\n",
    "                             batch['skg_embed2'].to(device),\\\n",
    "                             batch['ps_embed2'].to(device),\\\n",
    "                             batch['home_features2'][:,:kwargs['home_features_dim']].to(device),\\\n",
    "                             batch['home_features2'][:,kwargs['home_features_dim']].to(torch.long).to(device)]\n",
    "        # model forward pass\n",
    "        q_emb,p_emb = model(**kwargs1)\n",
    "        \n",
    "        # optimizer \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if loss_type ==  \"contrastive\" : # contrastive\n",
    "            batch_labels = torch.where(batch_labels == 0,-1,batch_labels) # for CosineEmbeddingloss()\n",
    "            loss = model._contrastive_loss( q_emb,p_emb,batch_labels )\n",
    "        else: #classification\n",
    "            scores = F.sigmoid( F.cosine_similarity( q_emb,p_emb ) ) # applying sigmoid to get score between 0 and 1\n",
    "            loss = model._classification_loss( scores,batch_labels.to(torch.float32)  )\n",
    "            \n",
    "        # model backward pass\n",
    "        loss.backward()\n",
    "        # wt updation\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        losses.append(loss.item())\n",
    "        counter += bsz\n",
    "        \n",
    "        for metric in metrics:\n",
    "            metric(q_emb,p_emb, batch_labels)\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            message = 'Train : {} [{}/{} ({:.0f}%)]\\tLoss: {:.4f}'.format(\n",
    "                epoch, counter,len_train_data,100.*counter/len_train_data, np.round(np.mean(losses),4)) # np.round(train_loss/counter,4)\n",
    "            for metric in metrics:\n",
    "                message += '\\t{}: {:.2f}'.format(metric.name(), metric.value())\n",
    "            \n",
    "            logging.info(message)\n",
    "            print(message)\n",
    "            losses = []\n",
    "            \n",
    "    train_loss /= (batch_idx+1)\n",
    "    return train_loss,metrics # avg train epoch loss - averaged across all batches\n",
    "    \n",
    "def test_one_epoch(epoch,test_loader,model,optimizer,val=True,**kwargs):\n",
    "    \n",
    "    metrics = kwargs['metrics']\n",
    "    with torch.no_grad():\n",
    "        # metrics = kwargs['metrics']\n",
    "        for metric in metrics:\n",
    "            metric.reset()\n",
    "            \n",
    "    # model = kwargs['model']\n",
    "    model.eval()\n",
    "    \n",
    "    log_interval = kwargs['log_interval']\n",
    "    # bsz = kwargs['bsz']\n",
    "    device = kwargs['device']\n",
    "    loss_type = kwargs['loss_type']\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx,batch in enumerate(test_loader):\n",
    "            kwargs1 = {'anchor':[],'engaged':[]}\n",
    "            \n",
    "            #labels\n",
    "            batch_labels = batch['label'].to(device)\n",
    "            bsz = len(batch_labels)\n",
    "            \n",
    "            ## anchor tower input\n",
    "            kwargs1['anchor']= [batch['text_embed1'].to(device),\\\n",
    "                                batch['img_embed1'].view(bsz,kwargs['n_img'],kwargs['image_embed_dim']).to(device),\\\n",
    "                                batch['skg_embed1'].to(device),\\\n",
    "                                batch['ps_embed1'].to(device),\\\n",
    "                                batch['home_features1'][:,:kwargs['home_features_dim']].to(device),\\\n",
    "                                batch['home_features1'][:,kwargs['home_features_dim']].to(torch.long).to(device)]\n",
    "            ## engaged tower input\n",
    "            kwargs1['engaged']= [batch['text_embed2'].to(device),\\\n",
    "                                 batch['img_embed2'].view(bsz,kwargs['n_img'],kwargs['image_embed_dim']).to(device),\\\n",
    "                                 batch['skg_embed2'].to(device),\\\n",
    "                                 batch['ps_embed2'].to(device),\\\n",
    "                                 batch['home_features2'][:,:kwargs['home_features_dim']].to(device),\\\n",
    "                                 batch['home_features2'][:,kwargs['home_features_dim']].to(torch.long).to(device)]\n",
    "\n",
    "            # model forward pass\n",
    "            q_emb,p_emb = model(**kwargs1)\n",
    "            \n",
    "            \n",
    "            if loss_type ==  \"contrastive\" : # contrastive\n",
    "                batch_labels = torch.where(batch_labels == 0,-1,batch_labels) # for CosineEmbeddingloss()\n",
    "                loss = model._contrastive_loss( q_emb,p_emb,batch_labels )\n",
    "            else: #classification\n",
    "                scores = F.sigmoid( F.cosine_similarity( q_emb,p_emb ) ) # applying sigmoid to get score between 0 and 1\n",
    "                loss = model._classification_loss( scores,batch_labels.to(torch.float32) )\n",
    "            \n",
    "            test_loss += loss.item()  # sum up batch loss\n",
    "            count += bsz\n",
    "            for metric in metrics:\n",
    "                metric(q_emb,p_emb, batch_labels)\n",
    "    test_loss /= (batch_idx+1)\n",
    "    return test_loss,metrics\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbc8c2d3-6c18-4e26-8bff-3ffaa1b5236d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ItemSageModel(\n",
       "  (loss): CosineEmbeddingLoss()\n",
       "  (zip_embed): Embedding(14, 16)\n",
       "  (home_linear): Linear(in_features=46, out_features=512, bias=True)\n",
       "  (image_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (text_linear): Linear(in_features=1536, out_features=512, bias=True)\n",
       "  (skg_linear): Linear(in_features=32, out_features=512, bias=True)\n",
       "  (ps_linear): Linear(in_features=128, out_features=512, bias=True)\n",
       "  (global_CLS_linear): Linear(in_features=1, out_features=512, bias=True)\n",
       "  (encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mlp_head): Sequential(\n",
       "    (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ItemSageModel(**model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c8ed045-56a9-45f6-bbaf-c8449899c62c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ###### USING CLASSIFICATION LOSS ######\n",
      " \n",
      "Train : 0 [128/1757020 (0%)]\tLoss: 0.9902\tAccuracy: 30.47\tF1-score: 0.47\n",
      "Train : 0 [166528/1757020 (9%)]\tLoss: 0.8181\tAccuracy: 49.29\tF1-score: 0.49\n",
      "Train : 0 [332928/1757020 (19%)]\tLoss: 0.7230\tAccuracy: 52.96\tF1-score: 0.49\n",
      "Train : 0 [499328/1757020 (28%)]\tLoss: 0.6688\tAccuracy: 56.64\tF1-score: 0.51\n",
      "Train : 0 [665728/1757020 (38%)]\tLoss: 0.6802\tAccuracy: 58.14\tF1-score: 0.51\n",
      "Train : 0 [832128/1757020 (47%)]\tLoss: 0.7245\tAccuracy: 57.88\tF1-score: 0.48\n",
      "Train : 0 [998528/1757020 (57%)]\tLoss: 0.7067\tAccuracy: 58.26\tF1-score: 0.47\n",
      "Train : 0 [1164928/1757020 (66%)]\tLoss: 0.6559\tAccuracy: 59.64\tF1-score: 0.49\n",
      "Train : 0 [1331328/1757020 (76%)]\tLoss: 0.6589\tAccuracy: 60.33\tF1-score: 0.51\n",
      "Train : 0 [1497728/1757020 (85%)]\tLoss: 0.6537\tAccuracy: 61.10\tF1-score: 0.51\n",
      "Train : 0 [1664128/1757020 (95%)]\tLoss: 0.6772\tAccuracy: 61.44\tF1-score: 0.51\n",
      "\n",
      "Epoch: 1/25. Train set: Average loss: 0.6966\tAccuracy: 61.45\tF1-score: 0.51\n",
      "Epoch: 1/25.00. Validation set: Average loss: 0.7150\tAccuracy: 59.19\tF1-score: 0.49\n",
      "Epoch time : 13.71 min. Model state saved to : /home/shreyasve/ItemSage_Data_v3/Models/model_classification_2023-07-07_18-05-44/model_state_epoch_1.pth\n",
      "Train : 1 [128/1757020 (0%)]\tLoss: 0.6925\tAccuracy: 64.06\tF1-score: 0.50\n",
      "Train : 1 [166528/1757020 (9%)]\tLoss: 0.6916\tAccuracy: 61.58\tF1-score: 0.45\n",
      "Train : 1 [332928/1757020 (19%)]\tLoss: 0.6721\tAccuracy: 62.83\tF1-score: 0.50\n",
      "Train : 1 [499328/1757020 (28%)]\tLoss: 0.6220\tAccuracy: 66.50\tF1-score: 0.56\n",
      "Train : 1 [665728/1757020 (38%)]\tLoss: 0.6081\tAccuracy: 69.48\tF1-score: 0.60\n",
      "Train : 1 [832128/1757020 (47%)]\tLoss: 0.6087\tAccuracy: 71.37\tF1-score: 0.63\n",
      "Train : 1 [998528/1757020 (57%)]\tLoss: 0.6070\tAccuracy: 72.60\tF1-score: 0.64\n",
      "Train : 1 [1164928/1757020 (66%)]\tLoss: 0.6055\tAccuracy: 73.53\tF1-score: 0.65\n",
      "Train : 1 [1331328/1757020 (76%)]\tLoss: 0.6001\tAccuracy: 74.19\tF1-score: 0.66\n",
      "Train : 1 [1497728/1757020 (85%)]\tLoss: 0.6016\tAccuracy: 74.77\tF1-score: 0.67\n",
      "Train : 1 [1664128/1757020 (95%)]\tLoss: 0.6018\tAccuracy: 75.26\tF1-score: 0.68\n",
      "\n",
      "Epoch: 2/25. Train set: Average loss: 0.6211\tAccuracy: 75.40\tF1-score: 0.68\n",
      "Epoch: 2/25.00. Validation set: Average loss: 0.6144\tAccuracy: 73.52\tF1-score: 0.68\n",
      "Epoch time : 13.56 min. Model state saved to : /home/shreyasve/ItemSage_Data_v3/Models/model_classification_2023-07-07_18-05-44/model_state_epoch_2.pth\n",
      "Train : 2 [128/1757020 (0%)]\tLoss: 0.6066\tAccuracy: 76.56\tF1-score: 0.70\n",
      "Train : 2 [166528/1757020 (9%)]\tLoss: 0.6124\tAccuracy: 76.59\tF1-score: 0.70\n",
      "Train : 2 [332928/1757020 (19%)]\tLoss: 0.6148\tAccuracy: 76.62\tF1-score: 0.70\n",
      "Train : 2 [499328/1757020 (28%)]\tLoss: 0.6111\tAccuracy: 76.28\tF1-score: 0.70\n",
      "Train : 2 [665728/1757020 (38%)]\tLoss: 0.6131\tAccuracy: 75.71\tF1-score: 0.69\n",
      "Train : 2 [832128/1757020 (47%)]\tLoss: 0.6129\tAccuracy: 75.88\tF1-score: 0.70\n",
      "Train : 2 [998528/1757020 (57%)]\tLoss: 0.6110\tAccuracy: 76.49\tF1-score: 0.70\n",
      "Train : 2 [1164928/1757020 (66%)]\tLoss: 0.6044\tAccuracy: 77.03\tF1-score: 0.71\n",
      "Train : 2 [1331328/1757020 (76%)]\tLoss: 0.5965\tAccuracy: 77.57\tF1-score: 0.71\n",
      "Train : 2 [1497728/1757020 (85%)]\tLoss: 0.5979\tAccuracy: 77.97\tF1-score: 0.72\n",
      "Train : 2 [1664128/1757020 (95%)]\tLoss: 0.6003\tAccuracy: 78.27\tF1-score: 0.72\n",
      "\n",
      "Epoch: 3/25. Train set: Average loss: 0.6068\tAccuracy: 78.41\tF1-score: 0.72\n",
      "Epoch: 3/25.00. Validation set: Average loss: 0.6011\tAccuracy: 79.93\tF1-score: 0.74\n",
      "Epoch time : 12.4 min. Model state saved to : /home/shreyasve/ItemSage_Data_v3/Models/model_classification_2023-07-07_18-05-44/model_state_epoch_3.pth\n",
      "Train : 3 [128/1757020 (0%)]\tLoss: 0.6051\tAccuracy: 78.12\tF1-score: 0.74\n",
      "Train : 3 [166528/1757020 (9%)]\tLoss: 0.5992\tAccuracy: 79.82\tF1-score: 0.74\n",
      "Train : 3 [332928/1757020 (19%)]\tLoss: 0.6002\tAccuracy: 80.01\tF1-score: 0.74\n",
      "Train : 3 [499328/1757020 (28%)]\tLoss: 0.5960\tAccuracy: 80.57\tF1-score: 0.75\n",
      "Train : 3 [665728/1757020 (38%)]\tLoss: 0.5959\tAccuracy: 80.99\tF1-score: 0.75\n",
      "Train : 3 [832128/1757020 (47%)]\tLoss: 0.5994\tAccuracy: 81.30\tF1-score: 0.75\n",
      "Train : 3 [998528/1757020 (57%)]\tLoss: 0.6006\tAccuracy: 81.51\tF1-score: 0.75\n",
      "Train : 3 [1164928/1757020 (66%)]\tLoss: 0.5979\tAccuracy: 81.82\tF1-score: 0.76\n",
      "Train : 3 [1331328/1757020 (76%)]\tLoss: 0.5921\tAccuracy: 82.11\tF1-score: 0.76\n",
      "Train : 3 [1497728/1757020 (85%)]\tLoss: 0.5946\tAccuracy: 82.42\tF1-score: 0.76\n",
      "Train : 3 [1664128/1757020 (95%)]\tLoss: 0.5991\tAccuracy: 82.52\tF1-score: 0.76\n",
      "\n",
      "Epoch: 4/25. Train set: Average loss: 0.5972\tAccuracy: 82.68\tF1-score: 0.77\n",
      "Epoch: 4/25.00. Validation set: Average loss: 0.5989\tAccuracy: 83.07\tF1-score: 0.77\n",
      "Epoch time : 12.28 min. Model state saved to : /home/shreyasve/ItemSage_Data_v3/Models/model_classification_2023-07-07_18-05-44/model_state_epoch_4.pth\n",
      "Train : 4 [128/1757020 (0%)]\tLoss: 0.6003\tAccuracy: 86.72\tF1-score: 0.82\n",
      "Train : 4 [166528/1757020 (9%)]\tLoss: 0.6016\tAccuracy: 82.33\tF1-score: 0.76\n",
      "Train : 4 [332928/1757020 (19%)]\tLoss: 0.5999\tAccuracy: 82.48\tF1-score: 0.76\n",
      "Train : 4 [499328/1757020 (28%)]\tLoss: 0.5972\tAccuracy: 82.90\tF1-score: 0.77\n",
      "Train : 4 [665728/1757020 (38%)]\tLoss: 0.5955\tAccuracy: 83.14\tF1-score: 0.77\n",
      "Train : 4 [832128/1757020 (47%)]\tLoss: 0.5981\tAccuracy: 83.50\tF1-score: 0.77\n",
      "Train : 4 [998528/1757020 (57%)]\tLoss: 0.6016\tAccuracy: 83.39\tF1-score: 0.77\n",
      "Train : 4 [1164928/1757020 (66%)]\tLoss: 0.5990\tAccuracy: 83.47\tF1-score: 0.77\n",
      "Train : 4 [1331328/1757020 (76%)]\tLoss: 0.5938\tAccuracy: 83.62\tF1-score: 0.77\n",
      "Train : 4 [1497728/1757020 (85%)]\tLoss: 0.5944\tAccuracy: 83.69\tF1-score: 0.77\n",
      "Train : 4 [1664128/1757020 (95%)]\tLoss: 0.5979\tAccuracy: 83.73\tF1-score: 0.77\n",
      "\n",
      "Epoch: 5/25. Train set: Average loss: 0.5977\tAccuracy: 83.74\tF1-score: 0.78\n",
      "Epoch: 5/25.00. Validation set: Average loss: 0.5978\tAccuracy: 82.76\tF1-score: 0.77\n",
      "Epoch time : 12.25 min. Model state saved to : /home/shreyasve/ItemSage_Data_v3/Models/model_classification_2023-07-07_18-05-44/model_state_epoch_5.pth\n",
      "Train : 5 [128/1757020 (0%)]\tLoss: 0.6047\tAccuracy: 80.47\tF1-score: 0.75\n",
      "Train : 5 [166528/1757020 (9%)]\tLoss: 0.5974\tAccuracy: 83.93\tF1-score: 0.78\n",
      "Train : 5 [332928/1757020 (19%)]\tLoss: 0.6121\tAccuracy: 81.48\tF1-score: 0.75\n",
      "Train : 5 [499328/1757020 (28%)]\tLoss: 0.6202\tAccuracy: 79.00\tF1-score: 0.72\n",
      "Train : 5 [665728/1757020 (38%)]\tLoss: 0.5993\tAccuracy: 79.64\tF1-score: 0.73\n",
      "Train : 5 [832128/1757020 (47%)]\tLoss: 0.5998\tAccuracy: 80.49\tF1-score: 0.74\n",
      "Train : 5 [998528/1757020 (57%)]\tLoss: 0.6009\tAccuracy: 81.12\tF1-score: 0.74\n",
      "Train : 5 [1164928/1757020 (66%)]\tLoss: 0.5993\tAccuracy: 81.61\tF1-score: 0.75\n",
      "Train : 5 [1331328/1757020 (76%)]\tLoss: 0.5943\tAccuracy: 81.93\tF1-score: 0.75\n",
      "Train : 5 [1497728/1757020 (85%)]\tLoss: 0.5962\tAccuracy: 82.15\tF1-score: 0.76\n",
      "Train : 5 [1664128/1757020 (95%)]\tLoss: 0.5989\tAccuracy: 82.26\tF1-score: 0.76\n",
      "\n",
      "Epoch: 6/25. Train set: Average loss: 0.6015\tAccuracy: 82.29\tF1-score: 0.76\n",
      "Epoch: 6/25.00. Validation set: Average loss: 0.5980\tAccuracy: 83.10\tF1-score: 0.77\n",
      "Epoch time : 12.25 min. Model state saved to : /home/shreyasve/ItemSage_Data_v3/Models/model_classification_2023-07-07_18-05-44/model_state_epoch_6.pth\n",
      "Train : 6 [128/1757020 (0%)]\tLoss: 0.6085\tAccuracy: 83.59\tF1-score: 0.79\n",
      "Train : 6 [166528/1757020 (9%)]\tLoss: 0.5975\tAccuracy: 84.05\tF1-score: 0.78\n",
      "Train : 6 [332928/1757020 (19%)]\tLoss: 0.5987\tAccuracy: 83.29\tF1-score: 0.77\n",
      "Train : 6 [499328/1757020 (28%)]\tLoss: 0.5972\tAccuracy: 83.10\tF1-score: 0.77\n",
      "Train : 6 [665728/1757020 (38%)]\tLoss: 0.5961\tAccuracy: 83.20\tF1-score: 0.77\n",
      "Train : 6 [832128/1757020 (47%)]\tLoss: 0.5997\tAccuracy: 83.22\tF1-score: 0.77\n",
      "Train : 6 [998528/1757020 (57%)]\tLoss: 0.6008\tAccuracy: 83.28\tF1-score: 0.77\n",
      "Train : 6 [1164928/1757020 (66%)]\tLoss: 0.5989\tAccuracy: 83.38\tF1-score: 0.77\n",
      "Train : 6 [1331328/1757020 (76%)]\tLoss: 0.5936\tAccuracy: 83.45\tF1-score: 0.77\n",
      "Train : 6 [1497728/1757020 (85%)]\tLoss: 0.5977\tAccuracy: 83.48\tF1-score: 0.77\n",
      "Train : 6 [1664128/1757020 (95%)]\tLoss: 0.6047\tAccuracy: 83.34\tF1-score: 0.77\n",
      "\n",
      "Epoch: 7/25. Train set: Average loss: 0.5985\tAccuracy: 83.27\tF1-score: 0.77\n",
      "Epoch: 7/25.00. Validation set: Average loss: 0.5970\tAccuracy: 83.73\tF1-score: 0.78\n",
      "Epoch time : 12.15 min. Model state saved to : /home/shreyasve/ItemSage_Data_v3/Models/model_classification_2023-07-07_18-05-44/model_state_epoch_7.pth\n",
      "Train : 7 [128/1757020 (0%)]\tLoss: 0.6122\tAccuracy: 84.38\tF1-score: 0.80\n",
      "Train : 7 [166528/1757020 (9%)]\tLoss: 0.6010\tAccuracy: 82.20\tF1-score: 0.76\n",
      "Train : 7 [332928/1757020 (19%)]\tLoss: 0.6019\tAccuracy: 82.27\tF1-score: 0.76\n",
      "Train : 7 [499328/1757020 (28%)]\tLoss: 0.5998\tAccuracy: 82.29\tF1-score: 0.76\n",
      "Train : 7 [665728/1757020 (38%)]\tLoss: 0.6005\tAccuracy: 82.18\tF1-score: 0.76\n",
      "Train : 7 [832128/1757020 (47%)]\tLoss: 0.6024\tAccuracy: 82.20\tF1-score: 0.76\n",
      "Train : 7 [998528/1757020 (57%)]\tLoss: 0.6028\tAccuracy: 82.26\tF1-score: 0.76\n",
      "Train : 7 [1164928/1757020 (66%)]\tLoss: 0.5998\tAccuracy: 82.35\tF1-score: 0.76\n",
      "Train : 7 [1331328/1757020 (76%)]\tLoss: 0.5945\tAccuracy: 82.49\tF1-score: 0.76\n",
      "Train : 7 [1497728/1757020 (85%)]\tLoss: 0.5957\tAccuracy: 82.61\tF1-score: 0.76\n",
      "Train : 7 [1664128/1757020 (95%)]\tLoss: 0.5984\tAccuracy: 82.70\tF1-score: 0.76\n",
      "\n",
      "Epoch: 8/25. Train set: Average loss: 0.5993\tAccuracy: 82.76\tF1-score: 0.76\n",
      "Epoch: 8/25.00. Validation set: Average loss: 0.5956\tAccuracy: 84.22\tF1-score: 0.78\n",
      "Epoch time : 12.16 min. Model state saved to : /home/shreyasve/ItemSage_Data_v3/Models/model_classification_2023-07-07_18-05-44/model_state_epoch_8.pth\n",
      "Train : 8 [128/1757020 (0%)]\tLoss: 0.6017\tAccuracy: 79.69\tF1-score: 0.75\n",
      "Train : 8 [166528/1757020 (9%)]\tLoss: 0.5976\tAccuracy: 83.36\tF1-score: 0.77\n",
      "Train : 8 [332928/1757020 (19%)]\tLoss: 0.5994\tAccuracy: 83.17\tF1-score: 0.77\n",
      "Train : 8 [499328/1757020 (28%)]\tLoss: 0.5970\tAccuracy: 83.15\tF1-score: 0.77\n",
      "Train : 8 [665728/1757020 (38%)]\tLoss: 0.5964\tAccuracy: 83.12\tF1-score: 0.77\n",
      "Train : 8 [832128/1757020 (47%)]\tLoss: 0.5995\tAccuracy: 83.12\tF1-score: 0.77\n",
      "Train : 8 [998528/1757020 (57%)]\tLoss: 0.6010\tAccuracy: 83.10\tF1-score: 0.77\n",
      "Train : 8 [1164928/1757020 (66%)]\tLoss: 0.5984\tAccuracy: 83.15\tF1-score: 0.77\n",
      "Train : 8 [1331328/1757020 (76%)]\tLoss: 0.5944\tAccuracy: 83.19\tF1-score: 0.77\n",
      "Train : 8 [1497728/1757020 (85%)]\tLoss: 0.5955\tAccuracy: 83.24\tF1-score: 0.77\n",
      "Train : 8 [1664128/1757020 (95%)]\tLoss: 0.5981\tAccuracy: 83.26\tF1-score: 0.77\n",
      "\n",
      "Epoch: 9/25. Train set: Average loss: 0.5975\tAccuracy: 83.28\tF1-score: 0.77\n",
      "Epoch: 9/25.00. Validation set: Average loss: 0.5960\tAccuracy: 83.79\tF1-score: 0.78\n",
      "Epoch time : 12.3 min. Model state saved to : /home/shreyasve/ItemSage_Data_v3/Models/model_classification_2023-07-07_18-05-44/model_state_epoch_9.pth\n",
      "Train : 9 [128/1757020 (0%)]\tLoss: 0.6070\tAccuracy: 78.91\tF1-score: 0.74\n",
      "Train : 9 [166528/1757020 (9%)]\tLoss: 0.5977\tAccuracy: 83.13\tF1-score: 0.77\n",
      "Train : 9 [332928/1757020 (19%)]\tLoss: 0.6001\tAccuracy: 82.88\tF1-score: 0.77\n",
      "Train : 9 [499328/1757020 (28%)]\tLoss: 0.5995\tAccuracy: 82.65\tF1-score: 0.76\n",
      "Train : 9 [665728/1757020 (38%)]\tLoss: 0.5985\tAccuracy: 82.45\tF1-score: 0.76\n",
      "Train : 9 [832128/1757020 (47%)]\tLoss: 0.6022\tAccuracy: 82.32\tF1-score: 0.76\n",
      "Train : 9 [998528/1757020 (57%)]\tLoss: 0.6029\tAccuracy: 82.22\tF1-score: 0.76\n",
      "Train : 9 [1164928/1757020 (66%)]\tLoss: 0.6004\tAccuracy: 82.22\tF1-score: 0.76\n",
      "Train : 9 [1331328/1757020 (76%)]\tLoss: 0.5950\tAccuracy: 82.27\tF1-score: 0.76\n",
      "Train : 9 [1497728/1757020 (85%)]\tLoss: 0.5959\tAccuracy: 82.31\tF1-score: 0.76\n",
      "Train : 9 [1664128/1757020 (95%)]\tLoss: 0.5982\tAccuracy: 82.34\tF1-score: 0.76\n",
      "\n",
      "Epoch: 10/25. Train set: Average loss: 0.5987\tAccuracy: 82.37\tF1-score: 0.76\n",
      "Epoch: 10/25.00. Validation set: Average loss: 0.5955\tAccuracy: 83.17\tF1-score: 0.77\n",
      "Epoch time : 12.1 min. Model state saved to : /home/shreyasve/ItemSage_Data_v3/Models/model_classification_2023-07-07_18-05-44/model_state_epoch_10.pth\n",
      "Train : 10 [128/1757020 (0%)]\tLoss: 0.5981\tAccuracy: 80.47\tF1-score: 0.76\n",
      "Train : 10 [166528/1757020 (9%)]\tLoss: 0.5975\tAccuracy: 82.25\tF1-score: 0.76\n",
      "Train : 10 [332928/1757020 (19%)]\tLoss: 0.5989\tAccuracy: 82.23\tF1-score: 0.76\n",
      "Train : 10 [499328/1757020 (28%)]\tLoss: 0.5964\tAccuracy: 82.29\tF1-score: 0.76\n",
      "Train : 10 [665728/1757020 (38%)]\tLoss: 0.5955\tAccuracy: 82.34\tF1-score: 0.76\n",
      "Train : 10 [832128/1757020 (47%)]\tLoss: 0.5983\tAccuracy: 82.45\tF1-score: 0.76\n",
      "Train : 10 [998528/1757020 (57%)]\tLoss: 0.5995\tAccuracy: 82.53\tF1-score: 0.76\n",
      "Train : 10 [1164928/1757020 (66%)]\tLoss: 0.5974\tAccuracy: 82.66\tF1-score: 0.76\n",
      "Train : 10 [1331328/1757020 (76%)]\tLoss: 0.5928\tAccuracy: 82.78\tF1-score: 0.77\n",
      "Train : 10 [1497728/1757020 (85%)]\tLoss: 0.5939\tAccuracy: 82.89\tF1-score: 0.77\n",
      "Train : 10 [1664128/1757020 (95%)]\tLoss: 0.5964\tAccuracy: 82.97\tF1-score: 0.77\n",
      "\n",
      "Epoch: 11/25. Train set: Average loss: 0.5964\tAccuracy: 83.01\tF1-score: 0.77\n",
      "Epoch: 11/25.00. Validation set: Average loss: 0.5953\tAccuracy: 83.50\tF1-score: 0.78\n",
      "Epoch time : 12.15 min. Model state saved to : /home/shreyasve/ItemSage_Data_v3/Models/model_classification_2023-07-07_18-05-44/model_state_epoch_11.pth\n",
      "Train : 11 [128/1757020 (0%)]\tLoss: 0.5883\tAccuracy: 78.91\tF1-score: 0.74\n",
      "Train : 11 [166528/1757020 (9%)]\tLoss: 0.5956\tAccuracy: 83.22\tF1-score: 0.77\n",
      "Train : 11 [332928/1757020 (19%)]\tLoss: 0.5967\tAccuracy: 83.20\tF1-score: 0.77\n",
      "Train : 11 [499328/1757020 (28%)]\tLoss: 0.5945\tAccuracy: 83.37\tF1-score: 0.77\n",
      "Train : 11 [665728/1757020 (38%)]\tLoss: 0.5946\tAccuracy: 83.40\tF1-score: 0.77\n",
      "Train : 11 [832128/1757020 (47%)]\tLoss: 0.5973\tAccuracy: 83.46\tF1-score: 0.77\n",
      "Train : 11 [998528/1757020 (57%)]\tLoss: 0.5986\tAccuracy: 83.53\tF1-score: 0.77\n",
      "Train : 11 [1164928/1757020 (66%)]\tLoss: 0.5962\tAccuracy: 83.61\tF1-score: 0.77\n",
      "Train : 11 [1331328/1757020 (76%)]\tLoss: 0.5918\tAccuracy: 83.68\tF1-score: 0.78\n",
      "Train : 11 [1497728/1757020 (85%)]\tLoss: 0.5929\tAccuracy: 83.73\tF1-score: 0.78\n",
      "Train : 11 [1664128/1757020 (95%)]\tLoss: 0.5962\tAccuracy: 83.76\tF1-score: 0.78\n",
      "\n",
      "Epoch: 12/25. Train set: Average loss: 0.5952\tAccuracy: 83.78\tF1-score: 0.78\n",
      "Epoch: 12/25.00. Validation set: Average loss: 0.5950\tAccuracy: 83.74\tF1-score: 0.78\n",
      "Epoch time : 12.18 min. Model state saved to : /home/shreyasve/ItemSage_Data_v3/Models/model_classification_2023-07-07_18-05-44/model_state_epoch_12.pth\n",
      "Train : 12 [128/1757020 (0%)]\tLoss: 0.5934\tAccuracy: 81.25\tF1-score: 0.76\n",
      "Train : 12 [166528/1757020 (9%)]\tLoss: 0.5952\tAccuracy: 83.43\tF1-score: 0.77\n",
      "Train : 12 [332928/1757020 (19%)]\tLoss: 0.5970\tAccuracy: 83.38\tF1-score: 0.77\n",
      "Train : 12 [499328/1757020 (28%)]\tLoss: 0.5952\tAccuracy: 83.44\tF1-score: 0.77\n",
      "Train : 12 [665728/1757020 (38%)]\tLoss: 0.5951\tAccuracy: 83.42\tF1-score: 0.77\n",
      "Train : 12 [832128/1757020 (47%)]\tLoss: 0.5981\tAccuracy: 83.42\tF1-score: 0.77\n",
      "Train : 12 [998528/1757020 (57%)]\tLoss: 0.5990\tAccuracy: 83.46\tF1-score: 0.77\n",
      "Train : 12 [1164928/1757020 (66%)]\tLoss: 0.5967\tAccuracy: 83.53\tF1-score: 0.77\n",
      "Train : 12 [1331328/1757020 (76%)]\tLoss: 0.5923\tAccuracy: 83.59\tF1-score: 0.77\n",
      "Train : 12 [1497728/1757020 (85%)]\tLoss: 0.5937\tAccuracy: 83.61\tF1-score: 0.77\n",
      "Train : 12 [1664128/1757020 (95%)]\tLoss: 0.5964\tAccuracy: 83.61\tF1-score: 0.77\n",
      "\n",
      "Epoch: 13/25. Train set: Average loss: 0.5956\tAccuracy: 83.62\tF1-score: 0.78\n",
      "Epoch: 13/25.00. Validation set: Average loss: 0.5949\tAccuracy: 83.51\tF1-score: 0.78\n",
      "Epoch time : 12.25 min. Model state saved to : /home/shreyasve/ItemSage_Data_v3/Models/model_classification_2023-07-07_18-05-44/model_state_epoch_13.pth\n",
      "Train : 13 [128/1757020 (0%)]\tLoss: 0.5947\tAccuracy: 76.56\tF1-score: 0.72\n",
      "Train : 13 [166528/1757020 (9%)]\tLoss: 0.5953\tAccuracy: 83.08\tF1-score: 0.77\n",
      "Train : 13 [332928/1757020 (19%)]\tLoss: 0.5973\tAccuracy: 82.99\tF1-score: 0.77\n",
      "Train : 13 [499328/1757020 (28%)]\tLoss: 0.5947\tAccuracy: 83.15\tF1-score: 0.77\n",
      "Train : 13 [665728/1757020 (38%)]\tLoss: 0.5946\tAccuracy: 83.18\tF1-score: 0.77\n",
      "Train : 13 [832128/1757020 (47%)]\tLoss: 0.5973\tAccuracy: 83.25\tF1-score: 0.77\n",
      "Train : 13 [998528/1757020 (57%)]\tLoss: 0.5979\tAccuracy: 83.35\tF1-score: 0.77\n",
      "Train : 13 [1164928/1757020 (66%)]\tLoss: 0.5950\tAccuracy: 83.50\tF1-score: 0.77\n",
      "Train : 13 [1331328/1757020 (76%)]\tLoss: 0.5905\tAccuracy: 83.62\tF1-score: 0.77\n",
      "Train : 13 [1497728/1757020 (85%)]\tLoss: 0.5919\tAccuracy: 83.70\tF1-score: 0.78\n",
      "Train : 13 [1664128/1757020 (95%)]\tLoss: 0.5945\tAccuracy: 83.75\tF1-score: 0.78\n",
      "\n",
      "Epoch: 14/25. Train set: Average loss: 0.5946\tAccuracy: 83.78\tF1-score: 0.78\n",
      "Epoch: 14/25.00. Validation set: Average loss: 0.5946\tAccuracy: 83.72\tF1-score: 0.78\n",
      "Epoch time : 12.18 min. Model state saved to : /home/shreyasve/ItemSage_Data_v3/Models/model_classification_2023-07-07_18-05-44/model_state_epoch_14.pth\n",
      "Early stopping triggered. No improvement in validation Accuracy. Best Epoch : 7\n"
     ]
    }
   ],
   "source": [
    "model = ItemSageModel(**model_args).to(model_args['device'])\n",
    "model_args['metrics'] = [AccumulatedAccuracyMetric(model_args['loss_margin']),AccumulatedF1Metric(model_args['loss_margin'])]\n",
    "lr = 1e-2\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
    "\n",
    "early_stopping_patience = 5  # Number of epochs without improvement before stopping\n",
    "best_validation_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "best_epoch = -1\n",
    "best_accuracy = 0.\n",
    "# model_save_dir = Path(os.path.join(model_args['model_path'],'_'.join(['model',datetime.now().strftime('%Y-%m-%d_%H-%M-%S')])))\n",
    "\n",
    "if model_args['loss_margin'] :\n",
    "    logging.info(\" ###### USING CONTRASTIVE LOSS ######\\n \")\n",
    "    print( \" ###### USING CONTRASTIVE LOSS ######\\n \")\n",
    "else :\n",
    "    logging.info( \" ###### USING CLASSIFICATION LOSS ######\\n \")\n",
    "    print( \" ###### USING CLASSIFICATION LOSS ######\\n \")\n",
    "    \n",
    "for epoch in range(1,model_args['n_epochs']+1 ):\n",
    "    scheduler.step()\n",
    "    tic = time.time()\n",
    "    with DataLoader(make_reader(model_args['train_path'], num_epochs=1, seed=1, shuffle_rows=True),\n",
    "                    batch_size=model_args['bsz']) as train_loader:\n",
    "        epoch_train_loss,metrics = train_one_epoch(epoch,train_loader,model,optimizer,**model_args)\n",
    "    message = '\\nEpoch: {}/{}. Train set: Average loss: {:.4f}'.format(epoch , model_args['n_epochs'], epoch_train_loss)\n",
    "    \n",
    "    for metric in metrics:\n",
    "        message += '\\t{}: {:.2f}'.format(metric.name(), metric.value())\n",
    "    \n",
    "    with DataLoader(make_reader(model_args['val_path'], num_epochs=1, seed=1, shuffle_rows=True),\n",
    "                    batch_size=model_args['bsz']) as val_loader:\n",
    "        epoch_val_loss,metrics = test_one_epoch(epoch,val_loader,model,optimizer,**model_args)\n",
    "    message += '\\nEpoch: {}/{:.2f}. Validation set: Average loss: {:.4f}'.format(epoch , model_args['n_epochs'],epoch_val_loss)\n",
    "    \n",
    "    for metric in metrics:\n",
    "        message += '\\t{}: {:.2f}'.format(metric.name(), metric.value())\n",
    "    \n",
    "    # save epoch model states\n",
    "    model_filename = 'model_state_epoch_{}.pth'.format(epoch)\n",
    "    torch.save(model.state_dict(), os.path.join(model_save_dir,model_filename))\n",
    "    toc = time.time()\n",
    "    message += '\\nEpoch time : {} min. Model state saved to : {}'.format(np.round((toc-tic)/60,2),os.path.join(model_save_dir,model_filename))\n",
    "    \n",
    "    logging.info(message)\n",
    "    print(message)\n",
    "    \n",
    "    # Early Stopping based on train_loss - val_loss and val accuracy\n",
    "    acc = metrics[0].value()\n",
    "    # epoch_loss_diff = abs(epoch_train_loss - epoch_val_loss)\n",
    "    # if (epoch_loss_diff < min_diff_till_now) & (epoch_val_acc >= best_val_acc-1):\n",
    "    #     min_diff_till_now = epoch_loss_diff\n",
    "    #     best_val_acc = epoch_val_acc\n",
    "    #     epochs_without_improvement = 0\n",
    "    # else:\n",
    "    #     epochs_without_improvement += 1\n",
    "        \n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        best_epoch = epoch\n",
    "    elif epoch - best_epoch > early_stopping_patience:\n",
    "        # print(\"Early stopped training at epoch %d\" % epoch)\n",
    "        logging.info(\"Early stopping triggered. No improvement in validation Accuracy. Best Epoch : {}\".format(best_epoch))\n",
    "        print(\"Early stopping triggered. No improvement in validation Accuracy. Best Epoch : {}\".format(best_epoch))\n",
    "        break  # terminate the training loop\n",
    "        \n",
    "    # if epoch_val_loss < best_validation_loss:\n",
    "    #     best_validation_loss = epoch_val_loss\n",
    "    #     epochs_without_improvement = 0\n",
    "    # else:\n",
    "    #     epochs_without_improvement += 1\n",
    "    \n",
    "    # if epochs_without_improvement >= early_stopping_patience:\n",
    "    #     logging.info(\"Early stopping triggered. No improvement in validation loss.\")\n",
    "    #     print(\"Early stopping triggered. No improvement in validation loss.\")\n",
    "    #     break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7921d94d-412b-4469-971b-782886b4c3a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8bcc7d7-c2b9-43f5-b19f-89146f6eb979",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77d27208-8e56-47fb-bd00-701de2020373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13726"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = 1757020\n",
    "size//128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b75b66e-44e1-40c5-8174-e3e240c97c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1372"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "13726//10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab5c59a-39f2-402e-9b0f-14390f437823",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
